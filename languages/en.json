{
    "name_en": "English",
    "name_local": "English",
    "keys": {
        "\n                            TensorRT is an NVIDIA-specific optimization library. This will create a \".engine\" TensorRT model based on your model.\n                            This will take several minutes to create, and will only work with a limited subset of inputs.\n                            (For example, LoRA will not work, and resolution is constrained to the range you select).\n                            Therefore this should only be used for models you want to use frequently, especially for example models exposed as an external API.\n                        ": "",
        "\n                            The \"Utilities\" tab provides a variety of general utilities and tools in the subtabs above, and the quicktools below.\n                        ": "",
        "\n                            This is a tool to analyze CLIP tokenization for Stable Diffusion models.\n                            All current Stable Diffusion models use the same CLIP token set, so this applies to them all.\n                            Simply type some text in the box below to see how it gets tokenized.\n                            It will show each text-piece, and its numerical ID.\n                        ": "",
        "\n                            This is a tool to extract a LoRA from the difference between two models.\n                            \"Base\" should be whatever the common base model is, eg SDXL-1.0-Base.\n                            \"Other\" should be the unique model with information to extract into the LoRA.\n                            The closer Base is to Other, the less complex the LoRA's data will be, and the more likely it will work well with other models that were built off the same base.\n                            \n                            Note that LoRA Extraction is an imperfect partial process, and will lose some of the data that makes the model unique.\n                            Rank is a number indicating how much detail to try to save. Higher numbers result in bigger files, and only slightly more accurate matching. Small values (eg 16) are usually sufficient.\n                        ": "",
        "\n                            This is a tool to quickle convert legacy Pickle (.pt, .ckpt, .bin) files to modern Safetensors files.\n                            WARNING: Pickle files may contain malicious code. Do not use this tool or otherwise load pickle files unless you trust their source.\n                            The pickle files will be moved to a \"backups\" folder, and the safetensors files left in place where the pickles originally were.\n                            You may delete the backups folder after confirming the new safetensors files work as intended.\n                            Be aware that this may temporarily use a large amount of filespace.\n                            This may take some time to process.\n                            (You can continue using the UI as normal while this runs)\n                        ": "",
        "\n                            To use the Stability API in StableSwarmUI, you must set your key.\n                            To get your key, go here.\n                            \n                            Once you have your key, enter it below and click save:\n                            \n                            Save\n                            Remove\n                            Your key: (Unknown)\n                        ": "",
        "\n                        UnspecifiedV-PredictionEpsilonx0LCMVPred ZSNR\n                    ": "",
        "\n                 Enable In Simple Tab\n            ": "",
        "\n                 Use Image\n            ": "",
        "\n                (TODO: info n stuff here)\n                \n                \n                    Stability API Key\n                    \n                        \n                            To use the Stability API in StableSwarmUI, you must set your key.\n                            To get your key, go here.\n                            \n                            Once you have your key, enter it below and click save:\n                            \n                            Save\n                            Remove\n                            Your key: (Unknown)\n                        \n                    \n                \n            ": "",
        "(For API usage) If enabled, requests preview output from ControlNet and no image generation at all.": "",
        "[AutoWebUI] Sampler": "",
        "[Close]\n                    ": "",
        "[ComfyUI] Custom Workflow": "",
        "[DT] CFG Scale Minimum": "",
        "[DT] CFG Scale Mode": "",
        "[DT] Interpolate Phi": "",
        "[DT] Mimic Scale": "",
        "[DT] Mimic Scale Minimum": "",
        "[DT] Mimic Scale Mode": "",
        "[DT] Scaling Startpoint": "",
        "[DT] Scheduler Value": "",
        "[DT] Separate Feature Channels": "",
        "[DT] Threshold Percentile": "",
        "[DT] Variability Measure": "",
        "[Dynamic Thresholding]\n'phi' interpolation factor.\nInterpolates between original value and DT value, such that 0.0 = use original, and 1.0 = use DT.\n(This exists because RCFG is bad and so half-removing it un-breaks it - better to just not do RCFG).": "",
        "[Dynamic Thresholding]\nCFG Scale minimum value (for non-constant CFG mode).": "",
        "[Dynamic Thresholding]\nIf either scale scheduler is 'Power', this is the power factor.\nIf using 'repeating', this is the number of repeats per image. Otherwise, it does nothing.": "",
        "[Dynamic Thresholding]\nMimic Scale minimum value (for non-constant mimic mode).": "",
        "[Dynamic Thresholding]\nMimic Scale value (target for the CFG Scale recentering).": "",
        "[Dynamic Thresholding]\nMode for the CFG Scale scheduler.": "",
        "[Dynamic Thresholding]\nMode for the Mimic Scale scheduler.": "",
        "[Dynamic Thresholding]\nthresholding percentile. '1' disables, '0.95' is decent value for enabled.": "",
        "[Dynamic Thresholding]\nWhether to scale relative to the mean value or to zero.\nUse 'MEAN' normally. If you want RCFG logic, use 'ZERO'.": "",
        "[Dynamic Thresholding]\nWhether to separate the feature channels.\nNormally leave this on. I think it should be off for RCFG?": "",
        "[Dynamic Thresholding]\nWhether to use standard deviation ('STD') or thresholded absolute values ('AD').\nNormally use 'AD'. Use 'STD' if wanting RCFG logic.": "",
        "[FreeU] Apply To": "",
        "[FreeU] Block One": "",
        "[FreeU] Block Two": "",
        "[FreeU] Skip One": "",
        "[FreeU] Skip Two": "",
        "[Grid Gen] Presets": "",
        "[Grid Gen] Prompt Replace": "",
        "[Internal] Backend Type": "",
        "[SAPI] Engine": "",
        "[SAPI] Sampler": "",
        "AddDebugData": "",
        "Admin": "",
        "Advanced Model Addons": "",
        "Advanced Sampling": "",
        "Allow Reordering": "",
        "AllowedModels": "",
        "AllowedSettings": "",
        "AllowUnsafeOutpaths": "",
        "Alt Resolution Height Multiplier": "",
        "Alt-Diffusion": "",
        "Alternate Steps value for the refiner stage.": "",
        "AlwaysRefreshOnLoad": "",
        "Amount of blur to apply to the segment mask before using it.": "",
        "AppendUserNameToOutputPath": "",
        "Apply parameter presets to the image. Can use a comma-separated list to apply multiple per-cell, eg 'a, b || a, c || b, c'": "",
        "Are you sure you want to shut StableSwarmUI down?": "",
        "Are you sure you want to update and restart StableSwarmUI?": "",
        "Aspect Ratio": "",
        "Auto Swap To Images": "",
        "Auto Swap To Previews": "",
        "Auto WebUI": "",
        "AutoCompletionsSource": "",
        "Automatic Scorer": "",
        "AutoPullDevUpdates": "",
        "AutoUpdate": "",
        "Backends": "",
        "Backends are automatically assigned unique ports. This value selects which port number to start the assignment from.\nDefault is '7820'.": "",
        "BackendStartingPort": "",
        "Batch Size": "",
        "Batch size - generates more images at once on a single GPU.\nThis increases VRAM usage.\nMay in some cases increase overall speed by a small amount (runs slower to get the images, but slightly faster per-image).": "",
        "Big Cards": "",
        "Big Thumbnails": "",
        "Block1 multiplier value for FreeU.\nPaper recommends 1.1.": "",
        "Block2 multiplier value for FreeU.\nPaper recommends 1.2.": "",
        "Blur-sigma for Self-Attention Guidance.\nDefaults to 2.0.": "",
        "Browse Workflows": "",
        "Builder for output file paths. Can use auto-filling placeholders like '[model]' for the model name, '[prompt]' for a snippet of prompt text, etc.\nFull details in the docs: https://github.com/Stability-AI/StableSwarmUI/blob/master/docs/User%20Settings.md#path-format": "",
        "Can be enabled to cache certain backend data.\nFor example, with ComfyUI backends this will add an extended cache on the object_info data.\nDefaults to false.": "",
        "Cancel": "",
        "CanChangeModels": "",
        "Cards": "",
        "Cascade Latent Compression": "",
        "CFG Scale": "",
        "CFG Scale for Cond2-Negative in InstructPix2Pix (Edit) models.": "",
        "CheckForUpdates": "",
        "Checking for updates...": "",
        "Choose File": "",
        "Clear Batch": "",
        "Clear Images": "",
        "ClearSystemRAMAfterMinutes": "",
        "ClearVRAMAfterMinutes": "",
        "CLIP Stop At Layer": "",
        "CLIP Tokenization": "",
        "CLIP Tokenizer": "",
        "Close": "",
        "Comfy Workflow": "",
        "ComfyUI": "",
        "ComfyUI Advanced": "",
        "Comma-separated list of parameters to exclude from 'Reuse Parameters'.\nFor example, set 'model' to not copy the model, or 'model,refinermodel,videomodel' to really never copy any models.": "",
        "Connected Users": "",
        "Continue On Error": "",
        "ControlNet": "",
        "ControlNet End": "",
        "ControlNet Image Input": "",
        "ControlNet Model": "",
        "ControlNet Preprocessor": "",
        "ControlNet Preview Only": "",
        "ControlNet Start": "",
        "ControlNet Strength": "",
        "ControlNet Three": "",
        "ControlNet Three End": "",
        "ControlNet Three Image Input": "",
        "ControlNet Three Model": "",
        "ControlNet Three Preprocessor": "",
        "ControlNet Three Start": "",
        "ControlNet Three Strength": "",
        "ControlNet Two": "",
        "ControlNet Two End": "",
        "ControlNet Two Image Input": "",
        "ControlNet Two Model": "",
        "ControlNet Two Preprocessor": "",
        "ControlNet Two Start": "",
        "ControlNet Two Strength": "",
        "ControlNets": "",
        "Convert": "",
        "Convert ControlNets": "",
        "Convert Embeddings": "",
        "Convert LoRAs": "",
        "Convert Models": "",
        "Convert to FP16?": "",
        "Convert VAEs": "",
        "Core Parameters": "",
        "Count": "",
        "Create": "",
        "Create TensorRT Engine": "",
        "Current LoRAs: ": "",
        "Current presets": "",
        "DataPath": "",
        "Debug Regional Prompting": "",
        "Default settings for users (unless the user modifies them, if so permitted).\n(NOTE: Usually, don't edit this. Go to the 'User' tab to edit your User-Settings).": "",
        "DefaultSDv1VAE": "",
        "DefaultSDXLVAE": "",
        "DefaultUser": "",
        "DefaultUserRestriction": "",
        "Delay, in seconds, betweeen Generate Forever updates.\nIf the delay hits and a generation is still waiting, it will be skipped.\nDefault is 0.1 seconds.": "",
        "Delete Preset": "",
        "Depth of subfolders to show": "",
        "Depth: ": "",
        "Description:": "",
        "DisableInternalArgs": "",
        "Display Advanced Options?": "",
        "Display Normally-Hidden Options?": "",
        "Do Not Save": "",
        "DoBackendDataCache": "",
        "Download": "",
        "DPI": "",
        "Dry Run": "",
        "Dynamic Thresholding": "",
        "Edit Preset": "",
        "Embeddings": "",
        "Enable AITemplate": "",
        "Enable this workflow in the Simple tab": "",
        "Enable/disable image": "",
        "EnablePreviews": "",
        "End Steps Early": "",
        "Engine for StabilityAPI to use.": "",
        "Error": "",
        "Exact Backend ID": "",
        "Examples": "",
        "Export Presets": "",
        "ExtraArgs": "",
        "Extract LoRA": "",
        "Failed to get session ID after 3 tries. Your account may have been invalidated. Try refreshing the page, or contact the site owner.": "",
        "Failed to get WebSocket address. You may be connecting to the server in an unexpected way. Please use \"http\" or \"https\" URLs.": "",
        "Failed to load ComfyUI Workflow backend. The server may still be loading.": "",
        "Failed to send request to server. Did the server crash?": "",
        "Fast Skip": "",
        "FileFormat": "",
        "Folder path for image output.\nIt is highly recommended that this is an empty folder.": "",
        "Folder path for input images.\nThis folder should contain a non-recursive single layer of image files (png/jpg).": "",
        "Format": "",
        "Free Memory": "",
        "FreeU": "",
        "Generate": "",
        "Generate Forever": "",
        "Generate Page": "",
        "Generate Previews": "",
        "GenerateForeverDelay": "",
        "Generic permission flags. '*' means all.\nDefault is all.": "",
        "Giant Thumbnails": "",
        "GLIGEN Model": "",
        "Global Region Factor": "",
        "Go back up 1 folder": "",
        "GPU_ID": "",
        "Grid Generator: Load Config": "",
        "Grid Generator: Save Config": "",
        "Height": "",
        "Hide Unaltered Parameters": "",
        "Higher values give the refiner more control, lower values give the base more control.\nThis is similar to 'Init Image Creativity', but for the refiner. This controls how many steps the refiner takes.": "",
        "Higher values make the ControlNet apply more strongly. Weaker values let the prompt overrule the ControlNet.": "",
        "Higher values make the generation more creative, lower values follow the init image closer.\nSometimes referred to as 'Denoising Strength' for 'img2img'.": "",
        "HintFormat": "",
        "Host": "",
        "How deeply to compress latents when using Stable Cascade.\nDefault is 32, you can get slightly faster but lower quality results by using 42.": "",
        "How likely an outdated image metadata entry is to be revalidated (ie have it's mtime checked against storage) each time an image's metadata is pulled.\nDefault 0.05 means 5% chance.\nSSD users can safely set it higher. HDD users may be happier setting it to 0.\nMetadata is always loaded the first time an image is seen.": "",
        "How long any one part can be.\nDefault is 40 characters.": "",
        "How long to wait (in seconds) after all queues are done before sending the queue end webhook.\nThis is useful to prevent rapid start+end calls.": "",
        "How many directories deep a user's custom OutPath can be.\nDefault is 5.": "",
        "How many frames to generate within the video.": "",
        "How many frames to interpolate between each frame in the video.\nHigher values are smoother, but make take significant time to save the output, and may have quality artifacts.": "",
        "How many images can try to be generating at the same time on this user.": "",
        "How many images the history view should scan server-side before deciding the list is sufficient for sorting. Not relevant when sorting by filename.": "",
        "How many images the history view should stop trying to load after.": "",
        "How many images to generate at once.": "",
        "How many minutes to wait after the last generation before automatically freeing up system RAM (to prevent issues with other programs).\nThis has the downside of causing models to fully load from data drive at next usage.\nUse a decimal number to free after seconds.\nDefaults to 60 minutes (one hour).": "",
        "How many minutes to wait after the last generation before automatically freeing up VRAM (to prevent issues with other programs).\nThis has the downside of a small added bit of time to load back onto VRAM at next usage.\nUse a decimal number to free after seconds.\nDefaults to 10 minutes.": "",
        "How many steps to use for the video model.\nHigher step counts yield better quality, but much longer generation time.\n40 well get good quality, but 20 is sufficient as a basis.": "",
        "How many times to retry initializing a backend before giving up. Default is 3.": "",
        "How many times to run the model.\nMore steps = better quality, but more time.\n20 is a good baseline for speed, 40 is good for maximizing quality.\nYou can go much higher, but it quickly becomes pointless above 70 or so.": "",
        "How much noise to add to the init image.\nHigher values yield more motion.": "",
        "How strong to apply ReVision image inputs.\nSet to 0 to disable ReVision processing.": "",
        "How strongly to apply the variation seed.\n0 = don't use, 1 = replace the base seed entirely. 0.5 is a good value.": "",
        "How strongly to scale prompt input.\nHigher CFG scales tend to produce more contrast, and lower CFG scales produce less contrast.\nToo-high values can cause corrupted/burnt images, too-low can cause nonsensical images.\n7 is a good baseline. Normal usages vary between 5 and 9.": "",
        "How to apply the refiner. Different methods create different results.\n'PostApply' runs the base in full, then runs the refiner with an Init Image.\n'StepSwap' swaps the model after x steps during generation.\n'StepSwapNoisy' is StepSwap but with first-stage noise only.": "",
        "How to display previews for generating videos.\n'Animate' shows a low-res animated video preview.\n'iterate' shows one frame at a time while it goes.\n'one' displays just the first frame.\n'none' disables previews.": "",
        "How to interpolate frames in the video.\n'RIFE' or 'FILM' are two different decent interpolation model options.": "",
        "How to launch the UI. If 'none', just quietly launch.\nIf 'web', launch your web-browser to the page.\nIf 'webinstall', launch web-browser to the install page.\nIf 'electron', launch the UI in an electron window (NOT YET IMPLEMENTED).": "",
        "How to process the mask.\n'Differential' = 'Differential Diffusion' technique, wherein the mask values are used as offsets for timestep of when to apply the mask or not.\n'Simple Latent' = the most basic latent masking technique.": "",
        "How to upscale the image, if upscaling is used.": "",
        "If checked, any usage of '<segment:>' syntax in prompts will save the generated mask in output.": "",
        "If checked, enables AITemplate for ComfyUI generations (UNet only). Only compatible with some GPUs.": "",
        "If checked, enables model-specific enhancements.\nFor example, on SDXL, smarter res-cond will be used.\nIf unchecked, will prefer more 'raw' behavior.": "",
        "If checked, intermediate images (eg before a refiner or segment stage) will be saved separately alongside the final image.": "",
        "If checked, outputs masks from regional prompting for debug reasons.": "",
        "If checked, shifts the empty latent to use a mean-average per-channel latent value (as calculated by Birchlabs).\nIf unchecked, default behavior of zero-init latents are used.\nThis can potentially improve the color range or even general quality on SDv1, SDv2, and SDXL models.\nNote that the effect is very minor.": "",
        "If checked, tells the server that previews are not desired.\nMay make generations slightly faster in some cases.": "",
        "If checked, tells the server to not save this image.\nUseful for quick test generations, or 'generate forever' usage.": "",
        "If checked, the seed will not be incremented when Images is above 1.\nUseful for example to test different wildcards for the same seed rapidly.": "",
        "If enabled and a mask is in use, this will recomposite the masked generated onto the original image for a cleaner result.\nIf disabled, VAE artifacts may build up across repeated inpaint operations.\nDefaults enabled.": "",
        "If enabled, automatically clears out the image batch when generating a new one. If disabled, leaves history to grow until you refresh the page.": "",
        "If enabled, automatically swaps to new images the moment they're done generating.": "",
        "If enabled, automatically swaps to previews of new images the moment they're available, before the image itself is done generating.": "",
        "If enabled, batch size will be reset to 1 when parameters are loaded.\nThis can prevent accidents that might thrash your GPU or cause compatibility issues, especially for example when importing a comfy workflow.\nYou can still set the batch size at will in the GUI.": "",
        "If enabled, decodes images through the VAE using tiles of this size.\nVAE Tiling reduces VRAM consumption, but takes longer and may impact quality.": "",
        "If enabled, feeds this prompt to an unsampler before resampling with your main prompt.\nThis is powerful for controlled image editing.\n\nFor example, use unsampler prompt 'a photo of a man wearing a black hat',\nand give main prompt 'a photo of a man wearing a sombrero', to change what type of hat a person is wearing.": "",
        "If enabled, removes the background from the generated image.\nThis uses RemBG.": "",
        "If enabled, the image will be shrunk to just the mask, and then grow by this value many pixels.\nAfter that, the generation process will run in full, and the image will be composited back into the original image at the end.\nThis allows for refining small details of an image for effectively.\nThis is also known as 'Inpaint Only Masked'.\nLarger values increase the surrounding context the generation receives, lower values contain it tighter and allow the AI to create more detail.": "",
        "If enabled, the mask will be blurred by this blur factor.\nThis makes the transition for the new image smoother.\nSet to 0 to disable.": "",
        "If set to non-0, adds DPI metadata to saved images.\n'72' is a good value for compatibility with some external software.": "",
        "If set to true, a '.txt' file will be saved alongside images with the image metadata easily viewable.\nThis can work even if saving in the image is disabled. Defaults disabled.": "",
        "If set true, new/upcoming/experimental features will be visible.": "",
        "If set true, some additional debugging data will be attached where relevant, such as in image metadata.": "",
        "If this is set to 'true', hides the installer page. If 'false', the installer page will be shown.": "",
        "If true, any time you load the UI, trigger a server refresh.\nIf false, only triggers a refresh if you restart Swarm or trigger a refresh manually from the Quick Tools menu.\nDefaults to true.": "",
        "If true, folders will be discard from starred image paths.": "",
        "If true, if the port is already in use, the server will try to find another port to use instead.\nIf false, the server will fail to start if the port is already in use.": "",
        "If true, logs will be saved to a file. If false, logs will be available in console and UI while running, but never saved to file.\nDefaults to false.\nMust restart Swarm to apply.": "",
        "If true, model metadata is tracked on a per-folder basis. This is better for example if you copy model folders to different machines, or have symlinks to different instances, or etc.\nIf false, model metadata is tracked in the central data folder. This is better if you don't want stray files in your model folders, or if you have several Swarm instances running simultaneously.": "",
        "If true, Swarm will automatically download and apply any development version updates as soon as they're available.\nDefaults to false.": "",
        "If true, Swarm will check if there's any updates available during startup. If false, it will not check for updates.\nUpdate check only downloads a simple JSON from GitHub to get the current version info, it does not transmit any telemetry nor does it download any files or apply the update.\nDefaults to true.": "",
        "If true, the Image History view will cache small preview thumbnails of images.\nThis should make things run faster. You can turn it off if you don't want that.": "",
        "If true, the user is treated as a full admin.\nThis includes the ability to modify these settings.": "",
        "If true, user may load models.\nIf false, they may only use already-loaded models.": "",
        "If true, when including model name (or loras or etc), the path will skip the folder (ie filename only).": "",
        "If you want to shut down the server, click the button below.": "",
        "Image aspect ratio. Some models can stretch better than others.": "",
        "Image Format": "",
        "Image height, in pixels.\nSDv1 uses 512, SDv2 uses 768, SDXL prefers 1024.\nSome models allow variation within a range (eg 512 to 768) but almost always want a multiple of 64.": "",
        "Image History": "",
        "Image seed.\n-1 = random.\nDifferent seeds produce different results for the same prompt.": "",
        "Image width, in pixels.\nSDv1 uses 512, SDv2 uses 768, SDXL prefers 1024.\nSome models allow variation within a range (eg 512 to 768) but almost always want a multiple of 64.": "",
        "Image-variation seed.\nCombined partially with the original seed to create a similar-but-different image for the same seed.\n-1 = random.": "",
        "ImageDataValidationChance": "",
        "ImageFormat": "",
        "ImageHistoryUsePreviews": "",
        "Images": "",
        "Images to include with the prompt, for eg ReVision or UnCLIP.\nIf this parameter is visible, you've done something wrong - this parameter is tracked internally.": "",
        "Implements 'FreeU: Free Lunch in Diffusion U-Net'": "",
        "Import": "",
        "Import From Generate Tab": "",
        "Import Presets": "",
        "Info": "",
        "Init Image": "",
        "Init Image Creativity": "",
        "Init Image Recomposite Mask": "",
        "Init Image Reset To Norm": "",
        "Init-image, to edit an image using diffusion.\nThis process is sometimes called 'img2img' or 'Image To Image'.": "",
        "Input Folder": "",
        "Install TensorRT": "",
        "Interrupt All Sessions": "",
        "Interrupt Current Session": "",
        "IP-Adapter Weight": "",
        "IP2P CFG 2": "",
        "IsInstalled": "",
        "Language": "",
        "LaunchMode": "",
        "Like the input prompt text, but describe what NOT to generate.\nTell the AI things you don't want to see.": "",
        "List": "",
        "Local Network": "",
        "LogLevel": "",
        "Logs": "",
        "LogsPath": "",
        "LoRA Extractor": "",
        "LoRA Section Confinement": "",
        "LoRA Weights": "",
        "LoRAs": "",
        "LoRAs (Low-Rank-Adaptation Models) are a way to customize the content of a model without totally replacing it.\nYou can enable one or several LoRAs over top of one model.": "",
        "Makes the generated image seamlessly tileable (like a 3D texture would be).\nOptionally, can be tileable on only the X axis (horizontal) or Y axis (vertical).": "",
        "Manually force a specific exact backend (by ID #) to be used for this generation.": "",
        "Mask Behavior": "",
        "Mask Blur": "",
        "Mask Image": "",
        "Mask Shrink Grow": "",
        "Mask-image, white pixels are changed, black pixels are not changed, gray pixels are half-changed.": "",
        "MaxBackendInitAttempts": "",
        "MaxImagesInHistory": "",
        "MaxImagesInMiniGrid": "",
        "MaxImagesScannedInHistory": "",
        "Maximum sigma value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "MaxLenPerPart": "",
        "MaxOutPathDepth": "",
        "MaxRequestsForcedOrder": "",
        "MaxSimulPreviews": "",
        "MaxT2ISimultaneous": "",
        "MaxTimeoutMinutes": "",
        "Merges the init image towards the latent norm.\nThis essentially lets you boost 'init image creativity' past 1.0.\nSet to 0 to disable.": "",
        "Metadata Reset": "",
        "Minimum sigma value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "Missing Preview Preset": "",
        "Model": "",
        "Model Author": "",
        "Model Date": "",
        "Model description text...": "",
        "Model License": "",
        "Model Merged From": "",
        "Model Specific Enhancements": "",
        "Model Tags": "",
        "Model Trigger Phrase": "",
        "Model Usage Hint": "",
        "ModelMetadataPerFolder": "",
        "ModelPathsSkipFolders": "",
        "ModelRoot": "",
        "Models": "",
        "Negative Prompt": "",
        "Network": "",
        "No Previews": "",
        "No Seed Increment": "",
        "Notice: this is raw internal configuration of parameters. Don't mess with this unless you know what you're doing.": "",
        "NvidiaQueryRateLimitMS": "",
        "Only keep images with a generated score above this minimum.": "",
        "Only keep the best *this many* images in a batch based on scoring.\n(For example, if batch size = 8, and this value = 2, then 8 images will generate and will be scored, and the 2 best will be kept and the other 6 discarded.)": "",
        "Open Empty Image Editor": "",
        "Optional advanced way to manually specify raw resolutions, useful for grids.\nWhen enabled, this overrides the default width/height params.": "",
        "Optional field to type in any personal text note you want.\nThis will be stored in the image metadata.": "",
        "Optional internal parameter used to confine LoRAs to certain sections of generation (eg a 'segment' block).\nComma separated list of section IDs (0 to mean global).\nMust match the length of the LoRAs input.": "",
        "Optional override for the final image file format.": "",
        "Optional source file for auto-completion texts (inside Data/Autocompletions).\nSee docs/Autocompletions for info.": "",
        "Optional upscale of the image between the base and refiner stage.\nSometimes referred to as 'high-res fix'.\nSetting to '1' disables the upscale.": "",
        "Optional VAE replacement for the refiner stage.": "",
        "Optionally specify a (raw HTML) welcome message here. If specified, will override the automatic welcome messages.": "",
        "Optionally specify a distinct model to use for 'segment' values.": "",
        "Optionally use a GLIGEN model.\nGLIGEN is only compatible with SDv1 at time of writing.": "",
        "Optionally, type a negative prompt here...": "",
        "Options to override default VAEs with.": "",
        "Or you can update and restart if there are updates available:": "",
        "OutPathBuilder": "",
        "Output Folder": "",
        "Output Folder Name": "",
        "Output folder name...": "",
        "OutputPath": "",
        "OverQueue": "",
        "OverrideWelcomeMessage": "",
        "Overwrite Existing Files": "",
        "Overwrite existing presets": "",
        "Parameter Configuration": "",
        "Parameter for internally tracking YOLOv8 models.\nThis is not for real usage, it is just to expose the list to the UI handler.": "",
        "Parameters in the Preset": "",
        "Paths": "",
        "Percentage of steps to cut off before the image is done generation.": "",
        "Performance": "",
        "PermissionFlags": "",
        "PerRequestTimeoutMinutes": "",
        "Personal Note": "",
        "Pickle To Safetensors": "",
        "Port": "",
        "PortCanChange": "",
        "Prefix a line with # to make it a comment (ie won't be counted as an option).": "",
        "Preset description text...": "",
        "Presets": "",
        "Prompt": "",
        "Prompt Images": "",
        "Prompt only CSV": "",
        "Publish Generation Metadata": "",
        "QueueEndDelay": "",
        "QueueEndWebhook": "",
        "QueueStartWebhook": "",
        "Quick Tools": "",
        "Ratelimit, in milliseconds, between Nvidia GPU status queries. Default is 1000 ms (1 second).": "",
        "Raw Resolution": "",
        "Refiner": "",
        "Refiner Control Percentage": "",
        "Refiner HyperTile": "",
        "Refiner Method": "",
        "Refiner Model": "",
        "Refiner Steps": "",
        "Refiner Upscale": "",
        "Refiner Upscale Method": "",
        "Refiner VAE": "",
        "Refresh": "",
        "Regional Object Cleanup Factor": "",
        "Regional Object Inpainting Model": "",
        "Regional Prompting": "",
        "Reload Parameter Values": "",
        "Remove": "",
        "Remove Background": "",
        "Remove Workflow From Generate Tab": "",
        "Replace text in the prompt (or negative prompt) with some other text.": "",
        "Reset All Metadata": "",
        "Reset Page Layout": "",
        "Reset Params to Default": "",
        "ResetBatchSizeToOne": "",
        "Reshow Welcome Message": "",
        "Resolution": "",
        "Resolution, eg 1024x1024": "",
        "Resource Usage": "",
        "Restart All Backends": "",
        "Restrictions to apply to default users.": "",
        "ReuseParamExcludeList": "",
        "ReVision": "",
        "ReVision Model": "",
        "ReVision Strength": "",
        "ReVision Zero Prompt": "",
        "Rho value for the sampler.\nOnly applies to Karras/Exponential schedulers.": "",
        "Root path for data (user configs, etc).\nDefaults to 'Data'": "",
        "Root path for model files. Use a full-formed path (starting with '/' or a Windows drive like 'C:') to use an absolute path.\nDefaults to 'Models'.": "",
        "Root path for output files (images, etc).\nDefaults to 'Output'": "",
        "Safety check, the maximum duration all requests can be waiting for a backend before the system declares a backend handling failure.": "",
        "Sampler": "",
        "Sampler for StabilityAPI to use.": "",
        "Sampler Rho": "",
        "Sampler Sigma Max": "",
        "Sampler Sigma Min": "",
        "Sampler type (for AutoWebUI)": "",
        "Sampler type (for ComfyUI)\nGenerally, 'Euler' is fine, but for SD1 and SDXL 'dpmpp_2m' is popular when paired with the 'karras' scheduler.": "",
        "Sampling": "",
        "Save": "",
        "Save Custom Comfy Workflow": "",
        "Save Intermediate Images": "",
        "Save Segment Mask": "",
        "Save Workflow": "",
        "Save workflow...": "",
        "SaveFiles": "",
        "SaveLogToFile": "",
        "SaveMetadata": "",
        "SaveTextFileMetadata": "",
        "Scale for Self-Attention Guidance.\n''Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy.\nSpecifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteration and guides them accordingly.''\nDefaults to 0.5.": "",
        "Scheduler": "",
        "Scheduler type (for ComfyUI)\nGoes with the Sampler parameter above.": "",
        "Score Must Exceed": "",
        "Scoring": "",
        "Scoring engine(s) to use when scoring this image. Multiple scorers can be used and will be averaged together. Scores are saved in image metadata.": "",
        "SDClipVisionFolder": "",
        "SDControlNetsFolder": "",
        "SDEmbeddingFolder": "",
        "SDLoraFolder": "",
        "SDModelFolder": "",
        "SDVAEFolder": "",
        "Seamless Tileable": "",
        "Seed": "",
        "Seems like the ComfyUI workflow editor direct-access hasn't loaded. Give it a second to load - if it doesn't there might be an error message in your browser console or the server logs.": "",
        "Segment Mask Blur": "",
        "Segment Model": "",
        "Segmind Stable Diffusion 1B (SSD-1B)": "",
        "Select an IP-Adapter model to use IP-Adapter for image-prompt input handling.\nModels will automatically be downloaded when you first use them.\n<a href=\"https://github.com/Stability-AI/StableSwarmUI/blob/master/docs//Features/IPAdapter-ReVision.md\">See more docs here.</a>": "",
        "Select an IP-Adapter model to use IP-Adapter for image-prompt input handling.\nModels will automatically be downloaded when you first use them.\nSee more docs here.": "",
        "Select the checkbox next to inputs you want included in the preset.By default this will override values entirely. For text inputs, use {value} to include the current UI value instead of overriding.": "",
        "Self-Attention Guidance Scale": "",
        "Self-Attention Guidance Sigma Blur": "",
        "Server": "",
        "Server Configuration": "",
        "Server Info": "",
        "Set to a number above 1 to allow generations of multiple images to automatically generate square mini-grids when they're done.": "",
        "Settings related to backends.": "",
        "Settings related to file paths.": "",
        "Settings related to logging.": "",
        "Settings related to networking and the webserver.": "",
        "Settings related to output path building.": "",
        "Settings related to saved file format.": "",
        "Settings related to server performance.": "",
        "Settings related to the User Interface.": "",
        "Settings related to webhooks.": "",
        "Shifted Latent Average Init": "",
        "Show Advanced Backend Types": "",
        "Show Outputs": "",
        "Show Prompt Tokenization": "",
        "ShowExperimentalFeatures": "",
        "Shutdown": "",
        "Shutdown Server": "",
        "Simple": "",
        "Skip1 multiplier value for FreeU.\nPaper recommends 0.9.": "",
        "Skip2 multiplier value for FreeU.\nPaper recommends 0.2.": "",
        "Small Cards": "",
        "Small Thumbnails": "",
        "Stability API Key": "",
        "StabilityAPI": "",
        "Stable Cascade v1 (Stage A)": "",
        "Stable Cascade v1 (Stage B)": "",
        "Stable Cascade v1 (Stage C)": "",
        "Stable Diffusion 3 Medium": "",
        "Stable Diffusion v1": "",
        "Stable Diffusion v1 (Inpainting)": "",
        "Stable Diffusion v1 (TensorRT Engine)": "",
        "Stable Diffusion v1 ControlNet": "",
        "Stable Diffusion v1 Embedding": "",
        "Stable Diffusion v1 LoRA": "",
        "Stable Diffusion v1 VAE": "",
        "Stable Diffusion v2 (Depth)": "",
        "Stable Diffusion v2 (Inpainting)": "",
        "Stable Diffusion v2 (TensorRT Engine)": "",
        "Stable Diffusion v2 (Unclip)": "",
        "Stable Diffusion v2 Embedding": "",
        "Stable Diffusion v2 LoRA": "",
        "Stable Diffusion v2 Turbo": "",
        "Stable Diffusion v2-512": "",
        "Stable Diffusion v2-768v": "",
        "Stable Diffusion XL 0.9-Base": "",
        "Stable Diffusion XL 0.9-Base (TensorRT Engine)": "",
        "Stable Diffusion XL 0.9-Refiner": "",
        "Stable Diffusion XL 1.0 Edit": "",
        "Stable Diffusion XL 1.0-Base": "",
        "Stable Diffusion XL 1.0-Base (TensorRT Engine)": "",
        "Stable Diffusion XL 1.0-Base Control-LoRA": "",
        "Stable Diffusion XL 1.0-Base ControlNet": "",
        "Stable Diffusion XL 1.0-Base Embedding": "",
        "Stable Diffusion XL 1.0-Base LoRA": "",
        "Stable Diffusion XL 1.0-Base VAE": "",
        "Stable Diffusion XL 1.0-Refiner": "",
        "Stable Diffusion XL 1.0-Refiner (TensorRT Engine)": "",
        "Stable Diffusion XL Turbo": "",
        "Stable Diffusion XL Turbo (TensorRT Engine)": "",
        "Stable Video Diffusion Img2Vid 0.9": "",
        "Stable Video Diffusion Img2Vid v1": "",
        "Stable Video Diffusion Img2Vid v1 (TensorRT Engine)": "",
        "StableSwarmUI JSON": "",
        "StarNoFolders": "",
        "Start Anyway": "",
        "Start generating images\nRight-click for advanced options.": "",
        "StartScript": "",
        "Steps": "",
        "Swarm Internal": "",
        "Take Best N Score": "",
        "Tell the AI what you want to see, then press Enter to submit.\nConsider 'a photo of a cat', or 'cartoonish drawing of an astronaut'": "",
        "TensorRT support is not yet installed. You must install it to continue.": "",
        "Test Again": "",
        "Text filter, only show items that contain this text.": "",
        "The CFG Scale to use for video generation.\nVideos start with this CFG on the first frame, and then reduce to MinCFG (normally 1) by the end frame.\nSVD-XT normally uses 25 frames, and SVD (non-XT) 0.9 used 14 frames.": "",
        "The CLIP Vision model folder to use within 'ModelRoot'.\nDefaults to 'clip_vision'.\nAbsolute paths work too.": "",
        "The CLIP Vision model to use for ReVision inputs.\nThis will also override IPAdapter (if IPAdapter-G is in use).": "",
        "The ControlNet model to use.": "",
        "The ControlNets model folder to use within 'ModelRoot'.\nDefaults to 'controlnet'.\nAbsolute paths work too.": "",
        "The Embedding (eg textual inversion) model folder to use within 'ModelRoot'.\nDefaults to 'Embeddings'.\nAbsolute paths work too.": "",
        "The folder for wildcard (.txt) files, under Data.\nDefaults to 'Wildcards'": "",
        "The format for parameter hints to display as.\nDefault is 'BUTTON'.": "",
        "The FPS (frames per second) to use for video generation.\nThis configures the target FPS the video will try to generate for.": "",
        "The image to use as the input to ControlNet guidance.\nThis image will be preprocessed by the chosen preprocessor.\nIf ControlNet is enabled, but this input is not, Init Image will be used instead.": "",
        "The input prompt text that describes the image you want to generate.\nTell the AI what you want to see.": "",
        "The LoRA (or related adapter type) model folder to use within 'ModelRoot'.\nDefaults to 'Lora'.\nAbsolute paths work too.": "",
        "The maximum duration an individual request can be waiting on a backend to be available before giving up.\nNot to be confused with 'MaxTimeoutMinutes' which requires backends be unresponsive for that duration, this duration includes requests that are merely waiting because other requests are queued.\nDefaults to 60 * 24 * 7 = 1 week (ultra-long max queue duration).": "",
        "The maximum number of pending requests to continue forcing orderly processing of.\nOver this limit, requests may start going out of order.": "",
        "The minimum CFG to use for video generation.\nVideos start with max CFG on first frame, and then reduce to this CFG. Set to -1 to disable.": "",
        "The minimum tier of logs that should be visible in the console and saved to file.\nDefault is 'info'.": "",
        "The model folder to use within 'ModelRoot'.\nDefaults to 'Stable-Diffusion'.\nAbsolute paths work too.": "",
        "The model to use for refinement. This should be a model that's good at small-details, and use a structural model as your base model.\n'Use Base' will use your base model rather than switching.\nSDXL 1.0 released with an official refiner model.": "",
        "The model to use for video generation.\nThis should be an SVD (Stable Video Diffusion) model.\nNote that SVD favors a low CFG (~2.5).": "",
        "The path for where to store log file, parsed at time of program start, relative to the Data directory.\nMust restart Swarm to apply.\nCan use [year], [month], [month_name], [day], [day_name], [hour], [minute], [second], [pid].": "",
        "The preprocessor to use on the ControlNet input image.\nIf toggled off, will be automatically selected.\nUse 'None' to disable preprocessing.": "",
        "The server has updated since you opened the page, please refresh.": "",
        "The size of hypertiles to use for the refining stage.\nHyperTile is a technique to speed up sampling of large images by tiling the image and batching the tiles.\nThis is useful when using SDv1 models as the refiner. SDXL-Base models do not benefit as much.": "",
        "The VAE (autoencoder) model folder to use within 'ModelRoot'.\nDefaults to 'VAE'.\nAbsolute paths work too.": "",
        "The VAE (Variational Auto-Encoder) controls the translation between images and latent space.\nIf your images look faded out, or glitched, you may have the wrong VAE.\nAll models have a VAE baked in by default, this option lets you swap to a different one if you want to.": "",
        "Theme": "",
        "This server is likely accessible from LAN on one of the following addresses:": "",
        "Thumbnails": "",
        "Tools": "",
        "Try Again?": "",
        "Type": "",
        "UI": "",
        "Unsampler Prompt": "",
        "Update and Restart Server": "",
        "Upload Preset File (JSON or CSV)": "",
        "Use As ControlNet Input": "",
        "Use As Init": "",
        "Use As ReVision": "",
        "Use Inpainting Encode": "",
        "Use IP-Adapter": "",
        "Use Reference Only": "",
        "Use the 'Reference-Only' technique to guide the generation towards the input image.\nThis currently has side effects that notably prevent Batch from being used properly.": "",
        "Use This Workflow In Generate Tab": "",
        "User": "",
        "User Info": "",
        "User Settings": "",
        "Uses VAE Encode logic specifically designed for certain inpainting models.\nNotably this includes the RunwayML Stable-Diffusion-v1 Inpainting model.\nThis covers the masked area with gray.": "",
        "Utilities": "",
        "VAE": "",
        "VAE Tile Size": "",
        "VAEs": "",
        "Variation Seed": "",
        "Variation Seed Strength": "",
        "Video": "",
        "Video Augmentation Level": "",
        "Video Boomerang": "",
        "Video CFG": "",
        "Video Format": "",
        "Video FPS": "",
        "Video Frame Interpolation Method": "",
        "Video Frame Interpolation Multiplier": "",
        "Video Frames": "",
        "Video Min CFG": "",
        "Video Model": "",
        "Video Motion Bucket": "",
        "Video Preview Type": "",
        "Video Resolution": "",
        "Video Steps": "",
        "Webhook to call (empty JSON POST) when all queues are done and the server is going idle.\nLeave empty to disable any webhook.\nCall must return before queuing may restart.": "",
        "Webhook to call (empty JSON POST) when queues are starting up from idle.\nLeave empty to disable any webhook.\nCall must return before the first generation starts.": "",
        "WebHooks": "",
        "Weight to use with IP-Adapter (if enabled).": "",
        "Weight values for the LoRA model list.\nComma separated list of weight numbers.\nMust match the length of the LoRAs input.": "",
        "What custom workflow to use in ComfyUI (built in the Comfy Workflow Editor tab).\nGenerally, do not use this directly.": "",
        "What format to save images in.\nDefault is '.jpg' (at 100% quality).": "",
        "What format to save videos in.": "",
        "What language to display the UI in.\nDefault is 'en' (English).": "",
        "What layer of CLIP to stop at, from the end.\nAlso known as 'CLIP Skip'. Default CLIP Skip is -1 for SDv1, some models prefer -2.\nSDv2, SDXL, and beyond do not need this set ever.": "",
        "What main checkpoint model should be used.": "",
        "What models are allowed, as a path regex.\nDirectory-separator is always '/'. Can be '.*' for all, 'MyFolder/.*' for only within that folder, etc.\nDefault is all.": "",
        "What resolution/aspect the video should use.\n'Image Aspect, Model Res' uses the aspect-ratio of the image, but the pixel-count size of the model standard resolution.\n'Model Preferred' means use the model's exact resolution (eg 1024x576).\n'Image' means your input image resolution.": "",
        "What theme to use. Default is 'modern_dark'.": "",
        "What VAE to use with SDv1 models by default. Use 'None' to use the one in the model.": "",
        "What VAE to use with SDXL models by default. Use 'None' to use the one in the model.": "",
        "What web host address to use. `localhost` means your PC only.\nLinux users may use `0.0.0.0` to mean accessible to anyone that can connect to your PC (ie LAN users, or the public if your firewall is open).\nWindows users may use `*` for that, though it may require additional Windows firewall configuration.\nAdvanced server users may wish to manually specify a host bind address here.": "",
        "What web port to use. Default is '7801'.": "",
        "When enabled, the normal width parameter is used, and this value is multiplied by the width to derive the image height.": "",
        "When generating live previews, this is how many simultaneous generation requests can be waiting at one time.": "",
        "When to start applying controlnet, as a fraction of steps.\nFor example, 0.5 starts applying halfway through. Must be less than End.\nExcluding early steps reduces the controlnet's impact on overall image structure.": "",
        "When to stop applying controlnet, as a fraction of steps.\nFor example, 0.5 stops applying halfway through. Must be greater than Start.\nExcluding later steps reduces the controlnet's impact on finer details.": "",
        "When true, output paths always have the username as a folder.\nWhen false, this will be skipped.\nKeep this on in multi-user environments.": "",
        "When using an 'object' prompt, how much to cleanup the end result by.\nThis is the 'init image creativity' of the final cleanup step.\nSet to 0 to disable.": "",
        "When using regionalized prompts with distinct 'object' values, this overrides the model used to inpaint those objects.": "",
        "When using regionalized prompts, this factor controls how strongly the global prompt overrides the regional prompts.\n0 means ignore global prompt, 1 means ignore regional, 0.5 means half-n-half.": "",
        "Whether the '.' symbol can be used in OutPath - if enabled, users may cause file system issues or perform folder escapes.": "",
        "Whether to boomerang (aka pingpong) the video.\nIf true, the video will play and then play again in reverse to enable smooth looping.": "",
        "Whether to store metadata into saved images.\nDefaults enabled.": "",
        "Whether your files save to server data drive or not.": "",
        "Which models to apply FreeU to, as base, refiner, or both. Irrelevant when not using refiner.": "",
        "Which StableSwarmUI backend type should be used for this request.": "",
        "Which trained 'motion bucket' to use for the video model.\nHigher values induce more motion. Most values should stay in the 100-200 range.\n127 is a good baseline, as it is the most common value in SVD's training set.": "",
        "Which user-settings the user is allowed to modify.\nDefault is all of them.": "",
        "Width": "",
        "Wildcard Seed": "",
        "Wildcard selection seed.\nIf enabled, this seed will be used for selecting entries from wildcards.\nIf disabled, the image seed will be used.\n-1 = random.": "",
        "Wildcards": "",
        "Wildcards are lists of random prompt segments. One entry per line. Prompt sub-syntax is allowed (eg you can link another wildcard, or use <random:...>, or <preset:...> or anything else you want).": "",
        "WildcardsFolder": "",
        "YOLO Model Internal": "",
        "You can free up the VRAM usage, or system memory usage (cache) from backends with the two buttons below.": "",
        "Zeroes the prompt and negative prompt for ReVision inputs.\nApplies only to the base, the refiner will still get prompts.\nIf you want zeros on both, just delete your prompt text.\nIf not checked, empty prompts will be zeroed regardless.": ""
    }
}
